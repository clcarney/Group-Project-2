---
title: "Project Group 2 Loan Defaults Analysis"
author: "Chris Carney, Jae Woo Choi, Yi Han, Niklas Rikala, Lisa Stimpson"
date: "4/7/2018"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

###INTRO
  
  The objective of this analysis is to utilize data sourced from a peer-to-peer lending service to reliably determine when a borrower will default on a loan. In order to achieve this, the data must be examined, subjectivley and objectivley, with an understanding of loans and basic finance to determine which factors will and will not be valuable to predicting loan defults. The data consists of 52 variables regarding information about the borrowers current and past financial, credit, residential, and employment history.First we will examine and clean the data.

```{r}
library(dplyr)
loans<-read.csv('Listings2013.csv')
str(loans)
summary(loans)
#Duplicate Information
loans$loan_status<-NULL
loans$income_range<-NULL
#Non-sensical Values
loans$dti_wprosper_loan[loans$dti_wprosper_loan>1]<-1
loans$months_employed<-abs(loans$months_employed)
summary(loans$months_employed)
#Missing Values
loans<-na.omit(loans) #Removed, only 0.2% of data set
#Reformatting
#Dates
library(lubridate)
loans$loan_origination_date<-mdy(loans$loan_origination_date)
loans$month<-month(loans$loan_origination_date)
loans$weekday<-factor(weekdays(loans$loan_origination_date))

loans$first_recorded_credit_line<-mdy(loans$first_recorded_credit_line)
loans$credit_age<-as.numeric(loans$loan_origination_date-loans$first_recorded_credit_line)
loans$loan_origination_date<-NULL
loans$first_recorded_credit_line<-NULL
#Factors
loans$listing_category_id<-factor(loans$listing_category_id)
loans$lender_indicator<-factor(loans$lender_indicator)
loans$listing_term<-factor(loans$listing_term)
#Remove Useless Data
loans$borrower_city<-NULL
loans$borrower_state<-NULL
#Create Default variable
loans$default<-ifelse(loans$loan_status_description=='DEFAULTED'|loans$loan_status_description=='CHARGEOFF',1,0)
#Remove in Progress Loans -- Provide no information about Succesfull Payment or Defualts
loans<-loans[!(loans$loan_status_description=='CURRENT'),]
#Remove "unknown" data at time of interest rate determination, and data that we cannot interpret
loans$number_of_days<-NULL
loans$loan_status_description<-NULL
loans$principal_balance<-NULL
loans$borrower_rate<-NULL
loans$listing_monthly_payment<-NULL
loans$prosper_rating<-NULL
loans$prosper_score<-NULL
loans$listing_category_id<-NULL


#Observe Skew
table(loans$default) #Defaults are only approx 4% of data --- We need to adjust for this.
loans_completed<-subset(loans,loans$default==0)
loansDefualted<-subset(loans,loans$default==1) #Only Defaulted Loans


#Rescale
normalize <- function(x) {
  return((x-min(x)) / (max(x) - min(x)))
}

loans <- as_data_frame(lapply(loans, function(x) {
  if((class(x[1]) != "numeric") & (class(x[1]) != "integer")) {
    return (x)
  }
  return(normalize(x))
}))
```
###DATA CLEANING
  After loading the data set and examining its structure and a summary of each variable some discrepancies were found. There was duplicate varaibles for the loan status and income range values, the versions that contained more complete information were then selected and the other verisons removed. There were non sensical values that needed to be investigated including values of 1000000 in DTI ratios (found to mean income not verifiable or income is 0 and therefore the ratio goes to infinity) and negative Months of employment for people listed as employed (no significant understanding found). Thus far, it has been decided that 1000000 is a reasonable representation of infinity and therefore it will remain (for now). To handle negative months of employment, the absolute value of these numbers was taken to remove thier negative effect (IDK if this is the right approach). Additionally, all data that was deemed "unknown" at time of assigning interest rate were removed. This includes, number of days, borrower rate, and monthly payment. Also removed were scores and rating provided by the loan service that might alter how other, "real" credit measure are interpreted in the model, these included propser rating, prosper score, and listing category ID. The Remaining data was then put into a logistic regression. 
```{r}
#Data Exploration
op<-par(mfrow=c(1,2))
plot(loansDefualted$scorex, main = "Default vs Scorex", xlab = "Scorex", ylab = "Number of Defaults", cex.names = 0.7) #Largest proportion of defaulted loans comes from Mid Score range, and then high scores posibbly becauase loans arent typically given to people with lower scores 
plot(loans$scorex, main = "Number of Loans vs Scorex", xlab = "Scorex", ylab = "Number of Loans", cex.names = 0.7) #Most loans are only for high credit score borrowers
par(op)

op<-par(mfrow=c(1,2))
plot(loansDefualted$income_range_description, main = "Default vs Income", xlab = "Income Range", ylab = "Number of Defaults") #Again, most defaults come from mid income range, is this because loans not typically given to low income?
plot(loans$income_range_description, main = "Number of Loans vs Income", xlab = "Income Range", ylab = "Number of Loans")#Confirmed, only loans given to higher income borrowers
par(op)

op<-par(mfrow=c(3,2))
plot(loansDefualted$employment_status_description, main = "Default vs Employment", xlab = "Employment Status", ylab = "Number of Defaults", cex.names = 0.8) #Highest priority of Defaulted Loans comes from Employed or Other
plot(loansDefualted$lender_indicator, main = "Default vs Lender", xlab = "Lender Role", names = c("No", "Yes"), ylab = "Number of Defaults") #Very Few Lenders default on loans, this will likely be a significant factor
hist(loansDefualted$bankcard_utilization, main = "Default vs Card Utilization", xlab = "Bank Card Utilization Rate", ylab = "Number of Defaults") #borrowers with higher card utilization make a higher proportion of defaulted laons
hist(loansDefualted$month, main = "Default vs Starting Month", xlab = "Starting Month", ylab = "Number of Defaults", xaxt = "n")%>%
axis(side = 1, at = seq(1, 12, by = 1), labels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
#September is a big month to start defaulted loans ... students?
hist(loansDefualted$credit_age, main = "Default vs Credit Age", xlab = "Credit Age in Days", ylab = "Number of Defaults") #Approximatley normally distributed with largest proportion around 5000 days of credit
par(op)
```

```{r}
#Linear Regression -- Copied From Class --- Need to Redo #Do we have to keep this?
listings<-read.csv('Listings2013.csv')
str(listings)
library(lubridate)
library(lmtest)

#Clean
listings$loan_status<-factor(listings$loan_status)
listings$loan_origination_date<-mdy(listings$loan_origination_date)
listings$occupation<-NULL   
listings$borrower_city<-NULL
listings$borrower_state<-NULL
listings$first_recorded_credit_line<-NULL
listings$principal_balance<-NULL
listings$is_homeowner<-as.numeric(listings$is_homeowner)
listings$income_verifiable<-as.numeric(listings$income_verifiable)

listings.m<-lm(formula = borrower_rate ~ . , data = listings)
summary(listings.m)

#Drop Insignificant Variables
attach(listings)
listings2<-subset(listings,select = -c(income_range,income_range_description,months_employed,current_delinquencies,delinquencies_last7_years,public_records_last12_months,credit_lines_last7_years,amount_delinquent,installment_balance,real_estate_balance,revolving_balance,real_estate_payment,revolving_available_percent,satisfactory_accounts,was_delinquent_derog,delinquencies_over60_days, loan_status_description,dti_wprosper_loan,number_of_days))

listings2.m<-lm(formula = borrower_rate ~ . , data = listings2)
summary(listings2.m)


```


```{r}
#Logistic Regression
library(lmtest)
library(caret)
loans.m1<-glm(formula = default ~ . , data = loans, family ='binomial')
summary(loans.m1) #Need to do some removing of conflicting variables

full<-glm(formula = default~.,data = loans, family = 'binomial')
null<-glm(formula = default~1,data = loans, family = 'binomial')
forwardSteps<-step(null,scope = list(lower = null, upper = full),data = loans, direction = 'forward')

loans2<-subset(loans,select = c(default, scorex, listing_term, dti_wprosper_loan, income_verifiable,
    income_range_description, lender_indicator, inquiries_last6_months,
    credit_lines_last7_years, total_open_revolving_accounts, employment_status_description, current_delinquencies,    public_records_last12_months, is_homeowner, amount_funded, delinquencies_over60_days,  
    installment_balance, stated_monthly_income, monthly_debt, open_credit_lines))

#Split Data - Make Bianry

set.seed(12345)
loans_rand<- loans2[order(runif(17941)), ]
loans_train<-loans_rand[1:12558,]
loans_test<-loans_rand[12559:17941,]
loans_train_labels<-loans_rand[1:12558,1]
loans_test_labels<-loans_rand[12559:17941,1]
table(loans_bi_test$default) 
table(loans_bi_train$default) 

loans.m2<-glm(formula = default ~., data = loans2_train, family = 'binomial') #Getting Perfect Separation??
summary(loans.m2)

logistic_prob<-predict(loans.m2,loans2_test, type = 'response')
logistic_prediction<-ifelse(logistic_prob>.2306,1,0)
confusionMatrix(logistic_prediction,loans2_test$default) 
```

###REGRESSION ANALYSIS
  the initial, full logistic regression resulted in few signifigant predictors, suggesting that certain variables be removed. While we had some knowledge of what financial predictors may be helpful in determining default probability, we determined it would be better to statistically evaluate significane of each predictor and remove variables as needed, then assess the validity of the result. A stepwise regression was then performed in both directions to determine the best predictors to include in the model to reach maximum significance. They included thirteen variables that encompassed properties of many other variables in the data set (i.e. income_range_description includes informaiton from occupation and stated income). This submodel also has the lowest AIC of any combination of variables in the original full model, suggesting it is a better indicator of default probability. Thes varibales were then transfered to a new data frame to be fed into preciditive models.

```{r}
#KNN --- Not a good model because binary data not good for establishing distance
library(gmodels)
library(class)
loans_knn<-as.numeric(knn(train = loans_bi_train,test = loans_bi_test, cl=loans_bi_train_labels))
loans_knn_pred<-ifelse()
confusionMatrix(loans_knn,loans_bi_test$default, positive = '1')

loans_bi<-as.data.frame(model.matrix(~.-1,data=loans))
loans_bi$default<-factor(loans_bi$default)
set.seed(12345)
loans_bi_rand<- loans_bi[order(runif(17941)), ]
loans_bi_train<-loans_bi_rand[1:12558,]
loans_bi_test<-loans_bi_rand[12559:17941,]
loans_bi_train_labels<-loans_bi_rand[1:12558,1]
loans_bi_test_labels<-loans_bi_rand[12559:17941,1]
table(loans_bi_test$default) 
table(loans_bi_train$default) 
```
KNN
  The KNN is not good because a large amount of our data is categorical and therefore binary when converted into dummy variables. Dummy varaibles are very bad for calculating the distances required by the KNN alogrithm and therfore we see very poor performance.

```{r}
#ANN -- Better Algorithm for Binary Inputs
loansDT1<-train(default ~ ., data = loans_bi_train, method='nnet', trace = 0 )
loansDT_prediction<-predict(loansDT1,loans_bi_test)
confusionMatrix(loansDT_prediction,loans_bi_test$default)

```




